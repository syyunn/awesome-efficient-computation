# awesome-efficient-computation
List of related articles about how-to-achieve equal/similar accuracy with less memory consumption in DL

### Store Less Activations
The Reversible Residual Network: Backpropagation Without Storing Activations 
> the activation storage requirements are independent of depth.

### Via Approximation
Reformer : REFORMER: THE EFFICIENT TRANSFORMER
> Approximate attention computation based on locality-sensitive hashing 
